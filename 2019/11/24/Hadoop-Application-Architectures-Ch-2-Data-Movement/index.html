<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hadoop Application Architectures Ch.2 Data Movement | Ansel&#39;s Note</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Ingestion Extraction  Data Ingestion ConsiderationsCommon data sources:  data management system such as relational databases logs, event data files from existing storage system  Considerations:  Time">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop Application Architectures Ch.2 Data Movement">
<meta property="og:url" content="https://weasellin.github.io/2019/11/24/Hadoop-Application-Architectures-Ch-2-Data-Movement/index.html">
<meta property="og:site_name" content="Ansel&#39;s Note">
<meta property="og:description" content="Ingestion Extraction  Data Ingestion ConsiderationsCommon data sources:  data management system such as relational databases logs, event data files from existing storage system  Considerations:  Time">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/load_sequentially.png">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/load_parallel.png">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_fan_in.png">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_split.png">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_partition.png">
<meta property="og:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/camus.png">
<meta property="article:published_time" content="2019-11-24T23:55:50.000Z">
<meta property="article:modified_time" content="2020-01-21T00:47:52.000Z">
<meta property="article:author" content="Ansel Lin">
<meta property="article:tag" content="reading_note">
<meta property="article:tag" content="data_engineering">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="hadoop_application_architectures">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/load_sequentially.png">
  
    <link rel="alternative" href="/atom.xml" title="Ansel&#39;s Note" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <section id="main"><article id="post-Hadoop-Application-Architectures-Ch-2-Data-Movement" class="article article-type-post" itemscope itemprop="blogPost">
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hadoop Application Architectures Ch.2 Data Movement
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2019/11/24/Hadoop-Application-Architectures-Ch-2-Data-Movement/" class="article-date">
  <time datetime="2019-11-24T23:55:50.000Z" itemprop="datePublished">2019-11-24</time>
</a>
      
    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <ul>
<li>Ingestion</li>
<li>Extraction</li>
</ul>
<h2 id="Data-Ingestion-Considerations"><a href="#Data-Ingestion-Considerations" class="headerlink" title="Data Ingestion Considerations"></a>Data Ingestion Considerations</h2><p>Common data sources:</p>
<ul>
<li>data management system such as relational databases</li>
<li>logs, event data</li>
<li>files from existing storage system</li>
</ul>
<p>Considerations:</p>
<ul>
<li>Timeliness of data ingestion and accessibility</li>
<li>Incremental updates</li>
<li>Data access and processing</li>
<li>Source system and data structure</li>
<li>Partitioning and splitting of data</li>
<li>Storage format</li>
<li>Data transformation</li>
</ul>
<h3 id="Timeliness-of-Data-Ingestion"><a href="#Timeliness-of-Data-Ingestion" class="headerlink" title="Timeliness of Data Ingestion"></a>Timeliness of Data Ingestion</h3><p>Timeliness classification:</p>
<ul>
<li>Macro batch (15 mins -)</li>
<li>Microbatch (2 mins -)</li>
<li>Near-Real-Time Decision Support (2 secs -)</li>
<li>Near-Real-Time Event Processing (100 msecs -)</li>
<li>Real Time</li>
</ul>
<p>System complexity, cost, disk or memory.</p>
<h3 id="Incremental-Updates"><a href="#Incremental-Updates" class="headerlink" title="Incremental Updates"></a>Incremental Updates</h3><ul>
<li><code>append</code><ul>
<li>notice the <em>small files problem</em>, may require periodic process to merge small files</li>
<li>write to</li>
</ul>
</li>
<li><code>update</code><ul>
<li>HDFS<ul>
<li><em>delta file</em> and <em>compaction job</em></li>
<li>only work for multi-minute timeliness intervals</li>
</ul>
</li>
<li>HBase<ul>
<li>milliseconds timeliness</li>
<li>8 - 10 times slower scan (compare to HDFS)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Access-Patterns"><a href="#Access-Patterns" class="headerlink" title="Access Patterns"></a>Access Patterns</h3><ul>
<li>scan: HDFS (supports memory cache from Hadoop 2.3.0)</li>
<li>random access: HBase</li>
<li>search: Solr</li>
</ul>
<table>
<thead>
<tr>
<th>Tool</th>
<th>Use cases</th>
<th>Storage device</th>
</tr>
</thead>
<tbody><tr>
<td>MapReduce</td>
<td>Large batch processes</td>
<td>HDFS preferred</td>
</tr>
<tr>
<td>Hive</td>
<td>Batch processing with SQL-like language</td>
<td>HDFS preferred</td>
</tr>
<tr>
<td>Pig</td>
<td>Batch processing with a data flow language</td>
<td>HDFS preferred</td>
</tr>
<tr>
<td>Spark</td>
<td>Fast interactive processing</td>
<td>HDFS preferred</td>
</tr>
<tr>
<td>Giraph</td>
<td>Batch graph processing</td>
<td>HDFS preferred</td>
</tr>
<tr>
<td>Impala</td>
<td>MPP style SQL</td>
<td>HDFS is preferred for most cases</td>
</tr>
<tr>
<td>HBase API</td>
<td>Atomic puts, gets, and deletes on record-level data</td>
<td>HBase</td>
</tr>
</tbody></table>
<h3 id="Original-Source-System-and-Data-Structure"><a href="#Original-Source-System-and-Data-Structure" class="headerlink" title="Original Source System and Data Structure"></a>Original Source System and Data Structure</h3><h5 id="READ-SPEED-OF-THE-DEVICES-ON-SOURCE-SYSTEMS"><a href="#READ-SPEED-OF-THE-DEVICES-ON-SOURCE-SYSTEMS" class="headerlink" title="READ SPEED OF THE DEVICES ON SOURCE SYSTEMS"></a>READ SPEED OF THE DEVICES ON SOURCE SYSTEMS</h5><blockquote>
<p>Disk I/O: often a major bottleneck in any processing pipeline.<br>Generally, with Hadoop weâ€™ll see read speeds of anywhere from 20 MBps to 100 MBps, and there are limitations on the motherboard or controller for reading from all the disks on the system.<br>To maximize read speeds, make sure to take advantage of as many disks as possible on the source system.<br>On a typical drive three threads is normally required to maximize throughput, although this number will vary.</p>
</blockquote>
<ul>
<li>to use as many as multiple disks</li>
<li>multi-threads</li>
</ul>
<h5 id="ORIGINAL-FILE-TYPE"><a href="#ORIGINAL-FILE-TYPE" class="headerlink" title="ORIGINAL FILE TYPE"></a>ORIGINAL FILE TYPE</h5><h5 id="COMPRESSION"><a href="#COMPRESSION" class="headerlink" title="COMPRESSION"></a>COMPRESSION</h5><h5 id="RELATIONAL-DATABASE-MANAGEMENT-SYSTEMS"><a href="#RELATIONAL-DATABASE-MANAGEMENT-SYSTEMS" class="headerlink" title="RELATIONAL DATABASE MANAGEMENT SYSTEMS"></a>RELATIONAL DATABASE MANAGEMENT SYSTEMS</h5><p>Apache Sqoop: a very rich tool with lots of options, but at the same time it is simple and easy to learn</p>
<ul>
<li>batch process: slow timeliness<ul>
<li>Sqoop</li>
</ul>
</li>
<li>data pipeline split: one to RDBMS, one to HDFS<ul>
<li>Flume or Kafka</li>
</ul>
</li>
<li>network limited: edge node<ul>
<li>RDBMS file dump</li>
</ul>
</li>
</ul>
<h5 id="STREAMING-DATA"><a href="#STREAMING-DATA" class="headerlink" title="STREAMING DATA"></a>STREAMING DATA</h5><h5 id="LOGFILES"><a href="#LOGFILES" class="headerlink" title="LOGFILES"></a>LOGFILES</h5><ul>
<li>anti-pattern: read the logfiles from disk as they are written<ul>
<li>because this is almost impossible to implement without losing data</li>
</ul>
</li>
<li>recommend: Flume or Kafka</li>
</ul>
<h3 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h3><p>Options:</p>
<ul>
<li>Transformation</li>
<li>Partitioning</li>
<li>Splitting</li>
</ul>
<p>For timeliness,</p>
<ul>
<li>batch transformation<ul>
<li>Hive, Pig, MapReduce, Spark</li>
<li>checkpoint for failure</li>
<li>all-or-nothing</li>
</ul>
</li>
<li>streaming ingestion<ul>
<li>Flume<ul>
<li>interceptors<ul>
<li>notice the performance issue, external call, GC, etc.</li>
</ul>
</li>
<li>selectors<ul>
<li>decide which of the roads of event data will go down</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Network-Bottlenecks"><a href="#Network-Bottlenecks" class="headerlink" title="Network Bottlenecks"></a>Network Bottlenecks</h3><ul>
<li>increase bandwidth</li>
<li>compress data</li>
</ul>
<h3 id="Network-Security"><a href="#Network-Security" class="headerlink" title="Network Security"></a>Network Security</h3><ul>
<li>encrypt with OpenSSL</li>
</ul>
<h3 id="Push-or-Pull"><a href="#Push-or-Pull" class="headerlink" title="Push or Pull"></a>Push or Pull</h3><p>requirements:</p>
<ul>
<li>Keeping track of what has been sent</li>
<li>Handling retries or failover options in case of failure</li>
<li>Being respectful of the source system that data is being ingested from</li>
<li>Access and security</li>
</ul>
<h5 id="SQOOP"><a href="#SQOOP" class="headerlink" title="SQOOP"></a>SQOOP</h5><p>a pull solution, requires</p>
<ul>
<li>connection information for the source database</li>
<li>one or more tables to extract</li>
<li>ensure at a defined extraction rate</li>
<li>scheduled to not interfere with the source systemâ€™s peak load time</li>
</ul>
<h5 id="FLUME"><a href="#FLUME" class="headerlink" title="FLUME"></a>FLUME</h5><ul>
<li>Log4J appender<ul>
<li>pushing events through a pipeline</li>
</ul>
</li>
<li>spooling directory source or the JMS source<ul>
<li>events are being pulled</li>
</ul>
</li>
</ul>
<h3 id="Failure-Handling"><a href="#Failure-Handling" class="headerlink" title="Failure Handling"></a>Failure Handling</h3><blockquote>
<p>Failure scenarios need to be documented, including failure delay expectations and how data loss will be handled.</p>
</blockquote>
<ul>
<li>File transfer<ul>
<li>using directories for different stages<ul>
<li><em>ToLoad</em></li>
<li><em>InProgress</em></li>
<li><em>Failure</em></li>
<li><em>Successful</em></li>
</ul>
</li>
</ul>
</li>
<li>Streaming ingestion<ul>
<li>areas for failure<ul>
<li>publisher failure</li>
<li>broker failure</li>
<li>consumer failure</li>
</ul>
</li>
<li>deduplication<ul>
<li>is a heavy performance cost</li>
<li>some other works<ul>
<li><a href="https://segment.com/blog/exactly-once-delivery/" target="_blank" rel="noopener">https://segment.com/blog/exactly-once-delivery/</a></li>
<li><a href="http://eng.tapjoy.com/blog-list/real-time-deduping-at-scale" target="_blank" rel="noopener">http://eng.tapjoy.com/blog-list/real-time-deduping-at-scale</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Level-of-Complexity"><a href="#Level-of-Complexity" class="headerlink" title="Level of Complexity"></a>Level of Complexity</h3><p>ease of use, ex. HDFS CLI, FUSE, or NFS</p>
<h2 id="Data-Ingestion-Options"><a href="#Data-Ingestion-Options" class="headerlink" title="Data Ingestion Options"></a>Data Ingestion Options</h2><p>In here,</p>
<ul>
<li>File transfer</li>
<li>Sqoop</li>
<li>Flume</li>
<li>Kafka</li>
</ul>
<h3 id="File-Transfers"><a href="#File-Transfers" class="headerlink" title="File Transfers"></a>File Transfers</h3><p>Hadoop clientâ€™s <code>hadoop fs -put</code> and <code>hadoop fs -get</code></p>
<ul>
<li>simplest</li>
<li>all-or-nothing batch processing</li>
<li>single-threaded</li>
<li>no transformations</li>
<li>any file types</li>
</ul>
<h5 id="HDFS-CLIENT-COMMANDS"><a href="#HDFS-CLIENT-COMMANDS" class="headerlink" title="HDFS CLIENT COMMANDS"></a>HDFS CLIENT COMMANDS</h5><ul>
<li>configurable replicas, common default 3</li>
<li>checksum file accompanies each block</li>
<li><em>double-hop</em> from a edge node due to some network policy</li>
</ul>
<h5 id="MOUNTABLE-HDFS"><a href="#MOUNTABLE-HDFS" class="headerlink" title="MOUNTABLE HDFS"></a>MOUNTABLE HDFS</h5><ul>
<li>allow to use common filesystem commands</li>
<li>not full POSIX semantic</li>
<li>not support random write</li>
<li>potential misuse: small files problem<ul>
<li>mitigated by Solr for indexing, HBase, container format as Avro</li>
</ul>
</li>
</ul>
<p>implementations example</p>
<ul>
<li>Fuse-DFS<ul>
<li>no need to modify the Linux kernel</li>
<li>multiple hops, poor consistency model</li>
<li>not yet production ready</li>
</ul>
</li>
<li>NFSv3<ul>
<li>scaling by Hadoop NFS Gateway nodes</li>
<li>recommended to use only in small, manual data transfers</li>
</ul>
</li>
</ul>
<h4 id="Considerations"><a href="#Considerations" class="headerlink" title="Considerations"></a>Considerations</h4><ul>
<li>single sink or multiple sinks</li>
<li>reliability or not</li>
<li>transformation or not</li>
</ul>
<h3 id="Sqoop-Batch-Transfer-Between-Hadoop-and-Relational-Databases"><a href="#Sqoop-Batch-Transfer-Between-Hadoop-and-Relational-Databases" class="headerlink" title="Sqoop: Batch Transfer Between Hadoop and Relational Databases"></a>Sqoop: Batch Transfer Between Hadoop and Relational Databases</h3><blockquote>
<p>When used for importing data into Hadoop, Sqoop generates map-only MapReduce jobs where each mapper connects to the database using a Java database connectivity (JDBC) driver, selects a portion of the table to be imported, and writes the data to HDFS.</p>
</blockquote>
<h5 id="CHOOSING-A-SPLIT-BY-COLUMN"><a href="#CHOOSING-A-SPLIT-BY-COLUMN" class="headerlink" title="CHOOSING A SPLIT-BY COLUMN"></a>CHOOSING A SPLIT-BY COLUMN</h5><ul>
<li><code>split-by</code></li>
<li><code>num-mappers</code></li>
</ul>
<h5 id="USING-DATABASE-SPECIFIC-CONNECTORS-WHENEVER-AVAILABLE"><a href="#USING-DATABASE-SPECIFIC-CONNECTORS-WHENEVER-AVAILABLE" class="headerlink" title="USING DATABASE-SPECIFIC CONNECTORS WHENEVER AVAILABLE"></a>USING DATABASE-SPECIFIC CONNECTORS WHENEVER AVAILABLE</h5><h5 id="USING-THE-GOLDILOCKS-METHOD-OF-SQOOP-PERFORMANCE-TUNING"><a href="#USING-THE-GOLDILOCKS-METHOD-OF-SQOOP-PERFORMANCE-TUNING" class="headerlink" title="USING THE GOLDILOCKS METHOD OF SQOOP PERFORMANCE TUNING"></a>USING THE GOLDILOCKS METHOD OF SQOOP PERFORMANCE TUNING</h5><ul>
<li>start with a very low number of mappers</li>
<li>gradually increase it to achieve a balance</li>
</ul>
<h5 id="LOADING-MANY-TABLES-IN-PARALLEL-WITH-FAIR-SCHEDULER-THROTTLING"><a href="#LOADING-MANY-TABLES-IN-PARALLEL-WITH-FAIR-SCHEDULER-THROTTLING" class="headerlink" title="LOADING MANY TABLES IN PARALLEL WITH FAIR SCHEDULER THROTTLING"></a>LOADING MANY TABLES IN PARALLEL WITH FAIR SCHEDULER THROTTLING</h5><ul>
<li><p>Load the tables sequentially<br><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/load_sequentially.png" alt=""></p>
</li>
<li><p>Load the tables in parallel (Fair Scheduler)<br><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/load_parallel.png" alt=""></p>
</li>
</ul>
<h5 id="DIAGNOSING-BOTTLENECKS"><a href="#DIAGNOSING-BOTTLENECKS" class="headerlink" title="DIAGNOSING BOTTLENECKS"></a>DIAGNOSING BOTTLENECKS</h5><ul>
<li>Network bandwidth<ul>
<li>likely to be either 1 GbE or 10 GbE (120 MBps or 1.2 GBps)</li>
</ul>
</li>
<li>RDBMS<ul>
<li>review the query generated by the mappers</li>
<li>in Sqoop incremental mode<ul>
<li>make sure using index</li>
</ul>
</li>
<li>ingest entire table<ul>
<li>full table scans are typically preferred</li>
</ul>
</li>
<li>monitor the database</li>
<li>schedule the execution time</li>
</ul>
</li>
<li>Data skew<ul>
<li>default splitting range evenly with min to max of split_by column values</li>
<li>choose <code>--split-by</code></li>
<li>define <code>--boundary-query</code></li>
</ul>
</li>
<li>Connector<ul>
<li>RDBMS-specific connector is preferred</li>
</ul>
</li>
<li>Hadoop<ul>
<li>check disk I/O, CPU utilization, and swapping on the DataNodes where the mappers are running</li>
</ul>
</li>
<li>Inefficient access path<ul>
<li>incredibly important the split column is either the <strong>partition key</strong> or has an <strong>index</strong></li>
<li>if no such column, then use only one mapper</li>
</ul>
</li>
</ul>
<h5 id="KEEPING-HADOOP-UPDATED"><a href="#KEEPING-HADOOP-UPDATED" class="headerlink" title="KEEPING HADOOP UPDATED"></a>KEEPING HADOOP UPDATED</h5><ul>
<li>small table<ul>
<li>just overwrite it</li>
</ul>
</li>
<li>large table<ul>
<li>delta<ul>
<li>Incremental Sequence ID</li>
<li>Timestamp</li>
</ul>
</li>
<li>write to a new directory<ul>
<li>check <code>{output_dir}/_SUCCESS</code></li>
</ul>
</li>
<li>compaction with <code>sqoop-merge</code><ul>
<li>sorted and partitioned data set could be optimized in merge, map-only job</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Flume-Event-Based-Data-Collection-and-Processing"><a href="#Flume-Event-Based-Data-Collection-and-Processing" class="headerlink" title="Flume: Event-Based Data Collection and Processing"></a>Flume: Event-Based Data Collection and Processing</h3><h5 id="FLUME-ARCHITECTURE"><a href="#FLUME-ARCHITECTURE" class="headerlink" title="FLUME ARCHITECTURE"></a>FLUME ARCHITECTURE</h5><p>[Flume Components]</p>
<p>Main components inside of the Flume agent JVM:</p>
<ul>
<li>Sources<ul>
<li>consume events from external sources and forward to channels</li>
<li>including AvroSource, SpoolDirectorySource, HTTPSource, and JMSSource</li>
</ul>
</li>
<li>Interceptors<ul>
<li>allow events to be intercepted and modified in flight</li>
<li>anything that can be implemented in a Java class</li>
<li>formatting, partitioning, filtering, splitting, validating, or applying metadata to events</li>
</ul>
</li>
<li>Selectors<ul>
<li>routing for events</li>
<li>fork to multiple channels, or send to a specific channel based on the event</li>
</ul>
</li>
<li>Channels<ul>
<li>store events until theyâ€™re consumed by a sink</li>
<li>memory channel, file channel, balancing performance with durability</li>
</ul>
</li>
<li>Sinks<ul>
<li>remove events from a channel and deliver to a destination</li>
</ul>
</li>
</ul>
<h5 id="FLUME-PATTERNS"><a href="#FLUME-PATTERNS" class="headerlink" title="FLUME PATTERNS"></a>FLUME PATTERNS</h5><h6 id="Fan-in"><a href="#Fan-in" class="headerlink" title="Fan-in"></a>Fan-in</h6><p>Agents on Hadoop edge nodes.</p>
<p><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_fan_in.png" alt=""></p>
<h6 id="Splitting-data-on-ingest"><a href="#Splitting-data-on-ingest" class="headerlink" title="Splitting data on ingest"></a>Splitting data on ingest</h6><p>Backup HDFS cluster for disaster recovery (DR).</p>
<p><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_split.png" alt=""></p>
<h6 id="Partitioning-data-on-ingest"><a href="#Partitioning-data-on-ingest" class="headerlink" title="Partitioning data on ingest"></a>Partitioning data on ingest</h6><p>Ex. partition events by timestamp.</p>
<p><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/flume_ingest_partition.png" alt=""></p>
<h6 id="Splitting-events-for-streaming-analytics"><a href="#Splitting-events-for-streaming-analytics" class="headerlink" title="Splitting events for streaming analytics"></a>Splitting events for streaming analytics</h6><p>Send to streaming such as Storm or Spark Streaming. (point a Flume Avro sink to Spark Streamingâ€™s Flume Stream)</p>
<h5 id="FILE-FORMATS"><a href="#FILE-FORMATS" class="headerlink" title="FILE FORMATS"></a>FILE FORMATS</h5><ul>
<li>Text files<ul>
<li>with container format SequenceFiles or Avro</li>
<li>Avro is preferred because<ul>
<li>it stores schema as part of the file,</li>
<li>and also compresses more efficiently.</li>
<li>providing better failure handling.</li>
</ul>
</li>
</ul>
</li>
<li>Columnar formats<ul>
<li>RCFile, ORC, or Parquet are also <strong>not</strong> well suited for Flume</li>
<li>more data lose risk due to batch processing</li>
</ul>
</li>
<li>Customized <em>event serializers</em><ul>
<li>override the EventSerializer interface to apply your own logic and create a custom output format</li>
</ul>
</li>
</ul>
<h5 id="RECOMMENDATIONS"><a href="#RECOMMENDATIONS" class="headerlink" title="RECOMMENDATIONS"></a>RECOMMENDATIONS</h5><h6 id="Flume-sources"><a href="#Flume-sources" class="headerlink" title="Flume sources"></a>Flume sources</h6><ul>
<li>Batch size<ul>
<li>notice of the network latency for sending acknowledge</li>
<li>start from 1,000</li>
</ul>
</li>
<li>Threads<ul>
<li>pushing source: add more clients or client threads</li>
<li>pulling source: configure more sources in the agent</li>
</ul>
</li>
</ul>
<h6 id="Flume-sinks"><a href="#Flume-sinks" class="headerlink" title="Flume sinks"></a>Flume sinks</h6><ul>
<li>Number of sinks<ul>
<li>channel to sink, is one-to-many</li>
<li>sink is single thread</li>
<li>limitation with more sinks should be the network or the CPU</li>
</ul>
</li>
<li>Batch Sizes<ul>
<li>overhead of an <em>fsync</em> system call</li>
<li>only big downside to large batches with a sink is an increased risk of duplicate events</li>
<li>balance between throughput and potential duplicates</li>
</ul>
</li>
</ul>
<h6 id="Flume-interceptors"><a href="#Flume-interceptors" class="headerlink" title="Flume interceptors"></a>Flume interceptors</h6><ul>
<li>capability to take an event or group of events and modify them, filter them, or split them</li>
<li>custom code comes with risk of issues like memory leaks or consuming excessive CPU</li>
</ul>
<h6 id="Flume-memory-channels"><a href="#Flume-memory-channels" class="headerlink" title="Flume memory channels"></a>Flume memory channels</h6><ul>
<li>if performance is your primary consideration, and data loss is not an issue</li>
<li>better for streaming analytics sink</li>
</ul>
<h6 id="Flume-file-channels"><a href="#Flume-file-channels" class="headerlink" title="Flume file channels"></a>Flume file channels</h6><ul>
<li>itâ€™s more durable than the memory channel</li>
<li>to use multiple disks</li>
<li>if using multiple file channels, use distinct directories, and preferably separate disks, for each channel</li>
<li>use dual checkpoint directories</li>
<li>better for persistent sink</li>
</ul>
<h6 id="JDBC-channels"><a href="#JDBC-channels" class="headerlink" title="JDBC channels"></a>JDBC channels</h6><ul>
<li>persists events to any JDBC-compliant data store</li>
<li>most durable channel, but also the least performant</li>
</ul>
<h6 id="Sizing-Channels"><a href="#Sizing-Channels" class="headerlink" title="Sizing Channels"></a>Sizing Channels</h6><ul>
<li>Memory channels<ul>
<li>can be fed by multiple sources</li>
<li>can be fetched from by multiple sinks</li>
<li>so for a pipeline, one channel in a node usually is enough</li>
</ul>
</li>
<li>Channel size<ul>
<li>large memory channel could have <strong>garbage collection</strong> activity that could slow down the whole agent</li>
</ul>
</li>
</ul>
<h5 id="FINDING-FLUME-BOTTLENECKS"><a href="#FINDING-FLUME-BOTTLENECKS" class="headerlink" title="FINDING FLUME BOTTLENECKS"></a>FINDING FLUME BOTTLENECKS</h5><ul>
<li>Latency between nodes<ul>
<li>batch size or more threads</li>
</ul>
</li>
<li>Throughput between nodes<ul>
<li>data compression</li>
</ul>
</li>
<li>Number of threads</li>
<li>Number of sinks</li>
<li>Channel</li>
<li>Garbage collection issues</li>
</ul>
<h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><ul>
<li><em>producers</em>, <em>brokers</em>, <em>consumers</em></li>
<li><em>topic</em>, <em>partition</em>, <em>offset</em></li>
</ul>
<p>Large number of partitions:</p>
<ul>
<li>Each partition can be consumed by at most one consumer from the same group</li>
<li>Therefore, we recommend at least as many partitions per node as there are servers in the cluster, possibly planning for a few years of growth.</li>
<li>There are no real downsides to having a few hundred partitions per topic.</li>
</ul>
<h5 id="KAFKA-FAULT-TOLERANCE"><a href="#KAFKA-FAULT-TOLERANCE" class="headerlink" title="KAFKA FAULT TOLERANCE"></a>KAFKA FAULT TOLERANCE</h5><ul>
<li><em>replica</em></li>
<li><em>leader</em>, <em>followers</em></li>
</ul>
<p>Producer acknowledge:</p>
<ul>
<li>all synchronized replicas</li>
<li>leader only</li>
<li>asynchronized</li>
</ul>
<p>Consumer only read <em>committed</em> (all synchronized) messages.</p>
<p>Supported semantics,</p>
<ul>
<li><em>at least once</em><ul>
<li>consumer advances the offset <strong>after</strong> processing the messages</li>
</ul>
</li>
<li><em>at most once</em><ul>
<li>consumer advances the offset <strong>before</strong> processing the messages</li>
</ul>
</li>
<li><em>exactly once</em><ul>
<li>consumer advances the offset, and processes the messages <strong>at the same time</strong> with two-phase commits</li>
</ul>
</li>
</ul>
<p>Multiple data centers deployment.</p>
<h5 id="KAFKA-AND-HADOOP"><a href="#KAFKA-AND-HADOOP" class="headerlink" title="KAFKA AND HADOOP"></a>KAFKA AND HADOOP</h5><table>
<thead>
<tr>
<th></th>
<th>Kafka</th>
<th>Flume</th>
</tr>
</thead>
<tbody><tr>
<td>Hadoop ingest solution</td>
<td>less</td>
<td>more</td>
</tr>
<tr>
<td>Required code writing</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td>Fault tolerant</td>
<td>higher</td>
<td>lower</td>
</tr>
<tr>
<td>Performance</td>
<td>higher</td>
<td>lower</td>
</tr>
</tbody></table>
<h6 id="Flume-with-Kafka"><a href="#Flume-with-Kafka" class="headerlink" title="Flume with Kafka"></a>Flume with Kafka</h6><ul>
<li>Kafka source<ul>
<li>consumer, reads data from Kafka and sends it to the Flume channel</li>
<li>adding multiple sources with the same <em>groupId</em> for load balancing and high availability</li>
<li>batch size tuning</li>
</ul>
</li>
<li>Kafka sink<ul>
<li>producer, sends data from a Flume channel to Kafka</li>
<li>batch size tuning</li>
</ul>
</li>
<li>Kafka channel<ul>
<li>combines a producer and a consumer</li>
<li>each batch will be sent to a separate Kafka partition, so the writes will be load-balanced</li>
</ul>
</li>
</ul>
<h6 id="Camus"><a href="#Camus" class="headerlink" title="Camus"></a>Camus</h6><ul>
<li>ingesting data from Kafka to HDFS</li>
<li>automatic discovery of Kafka topics from ZooKeeper</li>
<li>conversion of messages to Avro and Avro schema management</li>
<li>automatic partitioning</li>
<li>all-or-nothing batch processing</li>
<li>need to write decoder to convert Kafka messages to Avro</li>
</ul>
<p><img src="https://github.com/weasellin/docker-hexo/raw/master/source/_posts/Hadoop-Application-Architectures-Ch-2-Data-Movement/camus.png" alt=""></p>
<h2 id="Data-Extraction"><a href="#Data-Extraction" class="headerlink" title="Data Extraction"></a>Data Extraction</h2><ul>
<li>Moving data from Hadoop to an RDBMS or data warehouse<ul>
<li>In most cases, Sqoop will be the appropriate choice for ingesting the transformed data into the target database.</li>
</ul>
</li>
<li>Moving data between Hadoop clusters<ul>
<li>DistCp uses MapReduce to perform parallel transfers of large volumes of data.</li>
<li>DistCp is also suitable when either the source or target is a non-HDFS filesystemâ€”for example, an increasingly common need is to move data into a cloud-based system, such as Amazonâ€™s Simple Storage System (S3).</li>
</ul>
</li>
</ul>

      

      
        
    </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/12/04/Hadoop-Application-Architectures-Ch-3-Processing-Data-in-Hadoop/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hadoop Application Architectures Ch.3 Processing Data in Hadoop
        
      </div>
    </a>
  
  
    <a href="/2019/11/12/Hadoop-SequenceFile-Sync-Marker/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hadoop SequenceFile Sync Marker</div>
    </a>
  
</nav>

  
</article>


</section>
        <aside id="sidebar">
  <nav class="menus">
  	<ul>
  		<li><a href="/"><i class="icon icon-home"></i></a></li>
  		
			<li><a href="/archives"><i class="icon icon-fenlei"></i></a></li>
  		
  		
			<li><a href="/tags"><i class="icon icon-tag"></i></a></li>
  		
  		
  	</ul>
  </nav>
  <a id="go-top" href="#"><i class="icon icon-up"></i></a>
</aside>

      </div>
      <footer id="footer">
  
	<div id="footer-info" class="inner">
	  &copy; 2020 Ansel Lin 
	  - Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  - Theme <a href="https://github.com/hejianxian/hexo-theme-jane/" target="_blank">Jane</a>
	</div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/tags" class="mobile-nav-link">Tag</a>
  
    <a href="https://github.com/hejianxian" target="_blank" rel="noopener" class="mobile-nav-link">Github</a>
  
</nav>
    

<script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
</body>
</html>